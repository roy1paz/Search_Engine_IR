{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFqieKdvDbwE"
   },
   "outputs": [],
   "source": [
    "\n",
    "# !gcloud dataproc clusters list --region us-central1\n",
    "# Install a particular version of `google-cloud-storage` because (oddly enough) \n",
    "# the  version on Colab and GCP is old. A dependency error below is okay.\n",
    "# !pip install -q google-cloud-storage==1.43.0\n",
    "# !pip install -q graphframes\n",
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *\n",
    "# adding our python module to the cluster\n",
    "# sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "# sys.path.insert(0,SparkFiles.getRootDirectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AZsfml9DjSk"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Search_Engine.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "https://colab.research.google.com/drive/1mrdj_nSJjTsjf2rImUddPzBymbvdznmx\n",
    "\"\"\"\n",
    "\n",
    "# These will already be installed in the testing environment so disregard the\n",
    "# amount of time (~1 minute) it takes to install.\n",
    "import subprocess\n",
    "\n",
    "# if the following command generates an error, you probably didn't enable\n",
    "# the cluster security option \"Allow API access to all Google Cloud services\"\n",
    "# under Manage Security → Project Access when setting up the cluster\n",
    "# subprocess.run(['pip', 'install', '-U', '-q', 'PyDrive'])\n",
    "# subprocess.run(['apt', 'install', 'openjdk-8-jdk-headless', '-qq'])\n",
    "# subprocess.run(['pip', 'install', '-q', 'graphframes'])\n",
    "# subprocess.run(['pip', 'install', '-q', 'pyspark'])\n",
    "# subprocess.run(['pip', 'install', '-q', 'gcsfs'])\n",
    "\n",
    "# !pip install -U -q PyDrive\n",
    "# !pip install -q gcsfs\n",
    "\n",
    "\n",
    "# imports\n",
    "import sys\n",
    "from collections import Counter, OrderedDict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from timeit import timeit\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from graphframes import *\n",
    "import signal\n",
    "import gcsfs\n",
    "\n",
    "import hashlib\n",
    "import inverted_index_gcp\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# graphframes_jar = 'https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar'\n",
    "# spark_jars = '/usr/local/lib/python3.7/dist-packages/pyspark/jars'\n",
    "# subprocess.run(['wget', '-N', '-P', spark_jars, graphframes_jar])\n",
    "\n",
    "# client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OU0rgLmhDmhF"
   },
   "outputs": [],
   "source": [
    "## RENAME the project_id to yours project id from the project you created in GCP\n",
    "# Roy: custom-octagon-370512\n",
    "# Dudi: ir3-dudi-nassi\n",
    "# project_id = 'custom-octagon-370512'\n",
    "# subprocess.run(['gcloud', 'config', 'set', 'project', project_id])\n",
    "\n",
    "# data_bucket_name = 'wikidata20210801_preprocessed'\n",
    "# try:\n",
    "#     if os.environ[\"wikidata_preprocessed\"] is not None:\n",
    "#         pass\n",
    "# except:\n",
    "#     subprocess.run(['mkdir', 'wikidumps'])\n",
    "#     subprocess.run(['gsutil', '-u', project_id, 'cp', f'gs://{data_bucket_name}/*', 'wikidumps/'])\n",
    "\n",
    "# try:\n",
    "#     if os.environ[\"wikidata_preprocessed\"] is not None:\n",
    "#         path = os.environ[\"wikidata_preprocessed\"]+\"/wikidumps/*\"\n",
    "# except:\n",
    "#     path = \"wikidumps/*\"\n",
    "\n",
    "bucket_name = 'xxxxxx'\n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths=[]\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if b.name != 'graphframes.sh':\n",
    "        paths.append(full_path+b.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YBnKKwCDrAD"
   },
   "outputs": [],
   "source": [
    "parquetFile = spark.read.parquet(*paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJDEfOT6Dt9Y"
   },
   "outputs": [],
   "source": [
    "# parquetFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9kUhLe3DwrF"
   },
   "outputs": [],
   "source": [
    "doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd\n",
    "doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n",
    "pages_links = parquetFile.select(\"id\", \"anchor_text\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNy8w3llDyGf",
    "outputId": "4db076df-a36c-4a5d-fbcf-434c77b3b75f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "def token2bucket_id(token):\n",
    "    return int(_hash(token),16) % NUM_BUCKETS\n",
    "\n",
    "\n",
    "def partition_postings_and_write(postings,bucket_name):\n",
    "    # YOUR CODE HERE\n",
    "    def to_list(a):\n",
    "        return [a]\n",
    "\n",
    "    def append(a, b):\n",
    "        a.append(b)\n",
    "        return a\n",
    "\n",
    "    def extend(a, b):\n",
    "        a.extend(b)\n",
    "        return a\n",
    "    bucket_id_rdd = postings.map(lambda x: (token2bucket_id(x[0]), (x[0], x[1])))\n",
    "    write_list_rdd = bucket_id_rdd.combineByKey(to_list, append, extend).map(\n",
    "        lambda x: inverted_index_gcp.InvertedIndex.write_a_posting_list(x, bucket_name))\n",
    "    return write_list_rdd\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import operator\n",
    "from itertools import islice, count\n",
    "from contextlib import closing\n",
    "\n",
    "import json\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import builtins\n",
    "\n",
    "\n",
    "# TUPLE_SIZE = 6  # We're going to pack the doc_id and tf values in this\n",
    "# # many bytes.\n",
    "# TF_MASK = 2 ** 16 - 1  # Masking the 16 low bits of an integer\n",
    "\n",
    "# DL = {}  # We're going to update and calculate this after each document. This will be usefull for the calculation of AVGDL (utilized in BM25)\n",
    "\n",
    "\n",
    "from logging import exception\n",
    "\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "stopwords_frozen = frozenset(stopwords.words('english'))\n",
    "\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = stopwords_frozen.union(corpus_stopwords)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text: string , represting the text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    list of tokens (e.g., list of tokens).\n",
    "    \"\"\"\n",
    "    list_of_tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if\n",
    "                      token.group() not in all_stopwords]\n",
    "    return list_of_tokens\n",
    "\n",
    "\n",
    "def generate_query_tfidf_vector(query_to_search, index):\n",
    "    \"\"\"\n",
    "    Generate a vector representing the query. Each entry within this vector represents a tfidf score.\n",
    "    The terms representing the query will be the unique terms in the index.\n",
    "\n",
    "    We will use tfidf on the query as well.\n",
    "    For calculation of IDF, use log with base 10.\n",
    "    tf will be normalized based on the length of the query.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.').\n",
    "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
    "\n",
    "    index:           inverted index loaded from the corresponding files.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    vectorized query with tfidf scores\n",
    "    \"\"\"\n",
    "\n",
    "    epsilon = .0000001\n",
    "    total_vocab_size = len(index.term_total)\n",
    "    Q = np.zeros((total_vocab_size))\n",
    "    term_vector = list(index.term_total.keys())\n",
    "    counter = Counter(query_to_search)\n",
    "    for token in np.unique(query_to_search):\n",
    "        if token in index.term_total.keys():  # avoid terms that do not appear in the index.\n",
    "            tf = counter[token] / len(query_to_search)  # term frequency divded by the length of the query\n",
    "            df = index.df[token]\n",
    "            idf = math.log((len(index.DL)) / (df + epsilon), 10)  # smoothing\n",
    "\n",
    "            try:\n",
    "                ind = term_vector.index(token)\n",
    "                Q[ind] = tf * idf\n",
    "            except:\n",
    "                pass\n",
    "    return Q\n",
    "\n",
    "\n",
    "def get_posting_iter(index):\n",
    "    \"\"\"\n",
    "    This function returning the iterator working with posting list.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    index: inverted index\n",
    "    \"\"\"\n",
    "    words, pls = zip(*index.posting_lists_iter())\n",
    "    return words, pls\n",
    "\n",
    "\n",
    "def get_candidate_documents_and_scores(query_to_search, index, words, pls):\n",
    "    \"\"\"\n",
    "    Generate a dictionary representing a pool of candidate documents for a given query. This function will go through every token in query_to_search\n",
    "    and fetch the corresponding information (e.g., term frequency, document frequency, etc.') needed to calculate TF-IDF from the posting list.\n",
    "    Then it will populate the dictionary 'candidates.'\n",
    "    For calculation of IDF, use log with base 10.\n",
    "    tf will be normalized based on the length of the document.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.').\n",
    "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
    "\n",
    "    index:           inverted index loaded from the corresponding files.\n",
    "\n",
    "    words,pls: iterator for working with posting.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    dictionary of candidates. In the following format:\n",
    "                                                               key: pair (doc_id,term)\n",
    "                                                               value: tfidf score.\n",
    "    \"\"\"\n",
    "    candidates = {}\n",
    "    for term in np.unique(query_to_search):\n",
    "        if term in words:\n",
    "            list_of_doc = pls[words.index(term)]\n",
    "            normlized_tfidf = [(doc_id, (freq / index.DL[doc_id]) * math.log(len(index.DL) / index.df[term], 10)) for doc_id, freq\n",
    "                               in list_of_doc]\n",
    "\n",
    "            for doc_id, tfidf in normlized_tfidf:\n",
    "                candidates[(doc_id, term)] = candidates.get((doc_id, term), 0) + tfidf\n",
    "\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def generate_document_tfidf_matrix(query_to_search, index, words, pls):\n",
    "    \"\"\"\n",
    "    Generate a DataFrame `D` of tfidf scores for a given query.\n",
    "    Rows will be the documents candidates for a given query\n",
    "    Columns will be the unique terms in the index.\n",
    "    The value for a given document and term will be its tfidf score.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.').\n",
    "                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']\n",
    "\n",
    "    index:           inverted index loaded from the corresponding files.\n",
    "\n",
    "\n",
    "    words,pls: iterator for working with posting.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    DataFrame of tfidf scores.\n",
    "    \"\"\"\n",
    "\n",
    "    total_vocab_size = len(index.term_total)\n",
    "    candidates_scores = get_candidate_documents_and_scores(query_to_search, index, words,\n",
    "                                                           pls)  # We do not need to utilize all document. Only the docuemnts which have corrspoinding terms with the query.\n",
    "    unique_candidates = np.unique([doc_id for doc_id, freq in candidates_scores.keys()])\n",
    "    D = np.zeros((len(unique_candidates), total_vocab_size))\n",
    "    D = pd.DataFrame(D)\n",
    "\n",
    "    D.index = unique_candidates\n",
    "    D.columns = index.term_total.keys()\n",
    "\n",
    "    for key in candidates_scores:\n",
    "        tfidf = candidates_scores[key]\n",
    "        doc_id, term = key\n",
    "        D.loc[doc_id][term] = tfidf\n",
    "\n",
    "    return D\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "# def cosine_similarity(D, Q):\n",
    "#     \"\"\"\n",
    "#     Calculate the cosine similarity for each candidate document in D and a given query (e.g., Q).\n",
    "#     Generate a dictionary of cosine similarity scores\n",
    "#     key: doc_id\n",
    "#     value: cosine similarity score\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     D: DataFrame of tfidf scores.\n",
    "\n",
    "#     Q: vectorized query with tfidf scores\n",
    "\n",
    "#     Returns:\n",
    "#     -----------\n",
    "#     dictionary of cosine similarity score as follows:\n",
    "#                                                                 key: document id (e.g., doc_id)\n",
    "#                                                                 value: cosine similarty score.\n",
    "#     \"\"\"\n",
    "#     # YOUR CODE HERE\n",
    "#     d = {}\n",
    "#     for index, doc in D.iterrows():\n",
    "#         m = np.dot(doc, Q) / (norm(doc) * norm(Q))\n",
    "#         d[index] = m\n",
    "#     return d\n",
    "\n",
    "\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from scipy import sparse\n",
    "# def fast_cosine_similarity(D, Q):\n",
    "#   d=np.array(D)\n",
    "#   q=np.array(Q)\n",
    "#   d_sparse=sparse.csr_matrix(d)\n",
    "#   q_sparse=sparse.csr_matrix(q)\n",
    "#   return cosine_similarity(d_sparse,q_sparse)\n",
    "\n",
    "\n",
    "# Put your `generate_graph` function here\n",
    "def generate_graph(pages):\n",
    "  # YOUR CODE HERE\n",
    "  edges = pages.flatMapValues(lambda x: x).map(lambda x: (x[0],x[1][0])).distinct()\n",
    "  vertices1 = edges.values()\n",
    "  vertices2 = edges.keys()\n",
    "  vertices = vertices1.union(vertices2).distinct().map(lambda x: (x,x))\n",
    "  return edges, vertices\n",
    "\n",
    "\n",
    "def get_top_n(sim_dict, N=3):\n",
    "    \"\"\"\n",
    "    Sort and return the highest N documents according to the cosine similarity score.\n",
    "    Generate a dictionary of cosine similarity scores\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    sim_dict: a dictionary of similarity score as follows:\n",
    "                                                                key: document id (e.g., doc_id)\n",
    "                                                                value: similarity score. We keep up to 5 digits after the decimal point. (e.g., round(score,5))\n",
    "\n",
    "    N: Integer (how many documents to retrieve). By default N = 3\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    a ranked list of pairs (doc_id, score) in the length of N.\n",
    "    \"\"\"\n",
    "\n",
    "    return builtins.sorted([(doc_id, builtins.round(score, 5)) for doc_id, score in sim_dict.items()],\n",
    "                           key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "def get_topN_score_for_queries(queries_to_search, index, N=3):\n",
    "    \"\"\"\n",
    "    Generate a dictionary that gathers for every query its topN score.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    queries_to_search: a dictionary of queries as follows:\n",
    "                                                        key: query_id\n",
    "                                                        value: list of tokens.\n",
    "    index:           inverted index loaded from the corresponding files.\n",
    "    N: Integer. How many documents to retrieve. This argument is passed to the topN function. By default N = 3, for the topN function.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    return: a dictionary of queries and topN pairs as follows:\n",
    "                                                        key: query_id\n",
    "                                                        value: list of pairs in the following format:(doc_id, score).\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    dict1 = {}\n",
    "    d = get_posting_iter(index)\n",
    "    i = 0\n",
    "    for key, query in queries_to_search.items():\n",
    "        i += 1\n",
    "        Q = generate_query_tfidf_vector(query, index)\n",
    "        D = generate_document_tfidf_matrix(query, index, d[0], d[1])\n",
    "        x = fast_cosine_similarity(D, Q)\n",
    "        z = get_top_n(x, N)\n",
    "        dict1[key] = z\n",
    "\n",
    "    return dict1\n",
    "\n",
    "\n",
    "import math\n",
    "from itertools import chain\n",
    "import time\n",
    "\n",
    "\n",
    "# When preprocessing the data have a dictionary of document length for each document saved in a variable called `DL`.\n",
    "class BM25_from_index:\n",
    "    \"\"\"\n",
    "    Best Match 25.\n",
    "    ----------\n",
    "    k1 : float, default 1.5\n",
    "\n",
    "    b : float, default 0.75\n",
    "\n",
    "    index: inverted index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, index, k1=1.5, b=0.75):\n",
    "        self.b = b\n",
    "        self.k1 = k1\n",
    "        self.index = index\n",
    "        self.N = len(index.DL)\n",
    "        self.AVGDL = builtins.sum(index.DL.values()) / self.N\n",
    "        self.words, self.pls = zip(*self.index.posting_lists_iter())\n",
    "\n",
    "    def calc_idf(self, list_of_tokens):\n",
    "        \"\"\"\n",
    "        This function calculate the idf values according to the BM25 idf formula for each term in the query.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        idf: dictionary of idf scores. As follows:\n",
    "                                                    key: term\n",
    "                                                    value: bm25 idf score\n",
    "        \"\"\"\n",
    "        idf = {}\n",
    "        for term in list_of_tokens:\n",
    "            if term in self.index.df.keys():\n",
    "                n_ti = self.index.df[term]\n",
    "                idf[term] = math.log(1 + (self.N - n_ti + 0.5) / (n_ti + 0.5))\n",
    "            else:\n",
    "                pass\n",
    "        return idf\n",
    "\n",
    "    def search(self, queries, N=3):\n",
    "        \"\"\"\n",
    "        This function calculate the bm25 score for given query and document.\n",
    "        We need to check only documents which are 'candidates' for a given query.\n",
    "        This function return a dictionary of scores as the following:\n",
    "                                                                    key: query_id\n",
    "                                                                    value: a ranked list of pairs (doc_id, score) in the length of N.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
    "        doc_id: integer, document id.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        score: float, bm25 score.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        res = {}\n",
    "        # words, pls = get_posting_iter(self.index)\n",
    "        for query_id in queries:\n",
    "            self.idf = self.calc_idf(queries[query_id])\n",
    "            c = get_candidate_documents_and_scores(queries[query_id], self.index, self.words, self.pls)\n",
    "            cd = np.unique([doc[0] for doc in c.keys()])\n",
    "            sim = {doc: self._score(queries[query_id], doc) for doc in cd}\n",
    "            res[query_id] = get_top_n(sim, N)\n",
    "        return res\n",
    "\n",
    "    def _score(self, query, doc_id):\n",
    "        \"\"\"\n",
    "        This function calculate the bm25 score for given query and document.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        query: list of token representing the query. For example: ['look', 'blue', 'sky']\n",
    "        doc_id: integer, document id.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        score: float, bm25 score.\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        doc_len = self.index.DL[doc_id]\n",
    "        for term in query:\n",
    "            if term in self.index.term_total.keys():\n",
    "                term_frequencies = dict(self.pls[self.words.index(term)])\n",
    "                if doc_id in term_frequencies.keys():\n",
    "                    freq = term_frequencies[doc_id]\n",
    "                    numerator = self.idf[term] * freq * (self.k1 + 1)\n",
    "                    denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.AVGDL)\n",
    "                    score += (numerator / denominator)\n",
    "        return score\n",
    "        score = 0\n",
    "        # tune k1\n",
    "        # if len(query) > self.avgdl_:\n",
    "        #   K1 = self.k1 + 0.5\n",
    "        # elif len(query) == self.avgdl_:\n",
    "        #   K1 = self.k1 + 0.1\n",
    "        # else:\n",
    "        #   K1 = self.k1 + 0.3\n",
    "        K1 = self.k1\n",
    "        # tune b\n",
    "        doc_len_ = self.index.DL[doc_id]\n",
    "\n",
    "        B = 1 - self.b + self.b * (doc_len_/self.AVGDL)\n",
    "        # remove word that not in the corpus\n",
    "        for t in list(query):\n",
    "          if t not in self.df_.keys():\n",
    "            query.remove(t)\n",
    "        # calculate idf\n",
    "        idf_score = self.calc_idf(query)\n",
    "\n",
    "        #  calculate score\n",
    "        for t in query:\n",
    "          if self.tf_[doc_id][t] > 0:\n",
    "            tf_ij = ((K1 + 1) * self.tf_[doc_id][t]) / (B * K1 + self.tf_[doc_id][t])\n",
    "            score += tf_ij * idf_score[t]\n",
    "        return score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def merge_results(title_scores, body_scores, title_weight=0.5, text_weight=0.5, N=3):\n",
    "#     \"\"\"\n",
    "#     This function merge and sort documents retrieved by its weighte score (e.g., title and body).\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     title_scores: a dictionary build upon the title index of queries and tuples representing scores as follows:\n",
    "#                                                                             key: query_id\n",
    "#                                                                             value: list of pairs in the following format:(doc_id,score)\n",
    "\n",
    "#     body_scores: a dictionary build upon the body/text index of queries and tuples representing scores as follows:\n",
    "#                                                                             key: query_id\n",
    "#                                                                             value: list of pairs in the following format:(doc_id,score)\n",
    "#     title_weight: float, for weigted average utilizing title and body scores\n",
    "#     text_weight: float, for weigted average utilizing title and body scores\n",
    "#     N: Integer. How many document to retrieve. This argument is passed to topN function. By default N = 3, for the topN function.\n",
    "\n",
    "#     Returns:\n",
    "#     -----------\n",
    "#     dictionary of querires and topN pairs as follows:\n",
    "#                                                         key: query_id\n",
    "#                                                         value: list of pairs in the following format:(doc_id,score).\n",
    "#     \"\"\"\n",
    "\n",
    "#     # result: dict[query] = [(doc_id, title_score*weight + body_score*weight), (doc_id …)...]\n",
    "#     # YOUR CODE HERE\n",
    "#     def get_top_n_without_round(sim_dict, N=3):\n",
    "#         return sorted([(doc_id, score) for doc_id, score in sim_dict.items()], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#     weighted_dict = {query_id: {tup[0]: tup[1] * title_weight for tup in title_scores[query_id]} for query_id in\n",
    "#                      title_scores}\n",
    "\n",
    "#     for query_id in body_scores:\n",
    "#         for tup in body_scores[query_id]:\n",
    "#             doc_id = tup[0]\n",
    "#             text_weighted = tup[1] * text_weight\n",
    "#             if doc_id not in weighted_dict[query_id].keys():\n",
    "#                 weighted_dict[query_id][doc_id] = text_weighted\n",
    "#             else:\n",
    "#                 weighted_dict[query_id][doc_id] += text_weighted\n",
    "#         weighted_dict[query_id] = get_top_n_without_round(weighted_dict[query_id], N)\n",
    "#     return weighted_dict\n",
    "\n",
    "\n",
    "def calculate_df(postings):\n",
    "  # YOUR CODE HERE\n",
    "  result = postings.map(lambda x: (x[0],len(x[1])))\n",
    "  return result\n",
    "\n",
    "\n",
    "def get_id_name_list(half_and_half, doc_title_pairs_dict):\n",
    "    return [(str(tup[0]), doc_title_pairs_dict[tup[0]]) for query_id in half_and_half for tup in\n",
    "            half_and_half[query_id]]\n",
    "\n",
    "\n",
    "def tokenize_bin(text):\n",
    "    \"\"\"\n",
    "    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text: string , represting the text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    list of tokens (e.g., list of tokens).\n",
    "    \"\"\"\n",
    "    list_of_tokens = set([token.group() for token in RE_WORD.finditer(text.lower()) if\n",
    "                      token.group() not in all_stopwords])\n",
    "    return list(list_of_tokens)\n",
    "\n",
    "\n",
    "def reduce_word_counts(unsorted_pl):\n",
    "    lst = sorted(unsorted_pl, key=lambda x:x[0])\n",
    "    return lst\n",
    "\n",
    "\n",
    "def word_count(text, id):\n",
    "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "  # YOUR CODE HERE\n",
    "  tokens_without_stopword = [word for word in tokens if word not in all_stopwords]\n",
    "  result_dict = {}\n",
    "  if len(tokens_without_stopword) == 0:\n",
    "    tokens_without_stopword = tokens\n",
    "  for word in tokens_without_stopword:\n",
    "    if word not in result_dict:\n",
    "      result_dict[word] = [id,1]\n",
    "    else:\n",
    "      result_dict[word][1] += 1\n",
    "  map_reduce_res = [(key,tuple(result_dict[key])) for key in result_dict]\n",
    "  return map_reduce_res\n",
    "\n",
    "\n",
    "def word_count_bin(text, id):\n",
    "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
    "    lst_words = []\n",
    "    cnt = Counter(i for i in tokens if i not in all_stopwords)\n",
    "    if len(cnt) == 0:\n",
    "      cnt = Counter(i for i in tokens)\n",
    "    for i in cnt.keys():\n",
    "        lst_words.append((i,(id,1)))\n",
    "    return lst_words\n",
    "\n",
    "\n",
    "# Getting tokens from the text while removing punctuations.\n",
    "def stemmer_tokens(tokens):\n",
    "  # YOUR CODE HERE\n",
    "  stemmer = PorterStemmer()\n",
    "  tokens = tokens.split(\" \")\n",
    "  tokens_removed = [word for word in tokens if word not in all_stopwords]\n",
    "  if len(tokens_removed) == 0:\n",
    "    tokens_removed = tokens\n",
    "  tokens_stemmed = [stemmer.stem(word) for word in tokens_removed]\n",
    "  tokens_stemmed = ' '.join(tokens_stemmed)\n",
    "  return tokens_stemmed\n",
    "\n",
    "\n",
    "def fast_super_duper_cosine(query,index,words,pls):\n",
    "  scores = {}\n",
    "  for term in query:\n",
    "    list_of_candidates = []\n",
    "    if term in words:\n",
    "        list_of_doc = pls[words.index(term)]\n",
    "        normlized_tfidf = [(doc_id, (freq / index.DL[(doc_id)]) * math.log(len(index.DL) / index.df[term], 10)) for\n",
    "                            doc_id, freq in list_of_doc]\n",
    "        for i in range(len(list_of_doc)):\n",
    "          if list_of_doc[i][0] not in scores.keys():\n",
    "            scores[list_of_doc[i][0]] = normlized_tfidf[i][1] * list_of_doc[i][1]\n",
    "          else:\n",
    "            scores[list_of_doc[i][0]] += normlized_tfidf[i][1] * list_of_doc[i][1]\n",
    "  for doc_id in scores.keys():\n",
    "    scores[doc_id] = scores[doc_id]/index.DL[doc_id]\n",
    "  return sorted(scores.items(),key=lambda x: x[1],reverse=True)[:100]\n",
    "\n",
    "\n",
    "def merge_results(title_scores,body_scores,anchor_scores,title_weight=0.6,text_weight=0.3,anchor_weight=0.1, N=3):    \n",
    "    \"\"\"\n",
    "    This function merge and sort documents retrieved by its weighte score (e.g., title and body). \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    title_scores: a dictionary build upon the title index of queries and tuples representing scores as follows: \n",
    "                                                                            key: query_id\n",
    "                                                                            value: list of pairs in the following format:(doc_id,score)\n",
    "                \n",
    "    body_scores: a dictionary build upon the body/text index of queries and tuples representing scores as follows: \n",
    "                                                                            key: query_id\n",
    "                                                                            value: list of pairs in the following format:(doc_id,score)\n",
    "    title_weight: float, for weigted average utilizing title and body scores\n",
    "    text_weight: float, for weigted average utilizing title and body scores\n",
    "    N: Integer. How many document to retrieve. This argument is passed to topN function. By default N = 3, for the topN function. \n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    dictionary of querires and topN pairs as follows:\n",
    "                                                        key: query_id\n",
    "                                                        value: list of pairs in the following format:(doc_id,score). \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    def get_top_n_new(sim_dict,N):\n",
    "      return sorted([(doc_id,score) for doc_id, score in sim_dict.items()], key = lambda x: x[1],reverse=True)\n",
    "\n",
    "    d={}\n",
    "    res={}\n",
    "    for query_id,value in title_scores.items():\n",
    "      lst1=[(x[0],title_weight * x[1]) for x in value]\n",
    "      for doc_id,score in lst1:\n",
    "        d[doc_id]=score \n",
    "      value2=body_scores[query_id]\n",
    "      value3=anchor_scores[query_id]\n",
    "      lst2=[(x[0],text_weight * x[1]) for x in value2 ]\n",
    "      lst3=[(x[0],anchor_weight * x[1]) for x in value3]\n",
    "      for doc_id,score in lst2:\n",
    "          if(doc_id in d):\n",
    "            d[doc_id]+=score\n",
    "          else:\n",
    "            d[doc_id]=score\n",
    "      for doc_id,score in lst3:\n",
    "          if(doc_id in d):\n",
    "            d[doc_id]+=score\n",
    "          else:\n",
    "            d[doc_id]=score      \n",
    "      x=get_top_n_new(d,N)\n",
    "      res[query_id]=x\n",
    "      d={} \n",
    "    return res\n",
    "\n",
    "\n",
    "def search_binary(query,index,words,pls):\n",
    "  scores = {}\n",
    "  for term in query:\n",
    "    if term in words:\n",
    "        list_of_doc = pls[words.index(term)]\n",
    "        for doc_id, freq in list_of_doc:\n",
    "          if doc_id not in scores:\n",
    "            scores[doc_id] = 1\n",
    "          else:\n",
    "            scores[doc_id] += 1\n",
    "  return sorted(scores.items(),key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5YzGussD-_w"
   },
   "outputs": [],
   "source": [
    "def write_data_stemmed(name, doc_pairs, title_pairs):\n",
    "    # title\n",
    "    if name == 'title':\n",
    "        # bucket for title\n",
    "        bucket_name = 'title_bucket_roy_dudi_stemmed'\n",
    "        # convert rdd to list\n",
    "        doc_title_pairs_ = doc_pairs.collect()\n",
    "        # stemm\n",
    "        doc_pairs = doc_pairs.map(lambda x :(x[1],stemmer_tokens(x[0])))\n",
    "        # convert list to dict{id:text}\n",
    "        doc_title_pairs_dict = {element[1]: element[0] for element in doc_title_pairs_}\n",
    "        # tokenize the dicts\n",
    "        # doc_data_titles = {k: tokenize_bin(v) for k, v in doc_title_pairs_dict.items() if len(tokenize_bin(v)) != 0}        \n",
    "        # get posting list\n",
    "        word_counts = doc_pairs.flatMap(lambda x: word_count_bin(x[1], x[0]))\n",
    "        postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "        # partition posting lists and write out\n",
    "        _ = partition_postings_and_write(postings, bucket_name).collect()\n",
    "        # collect all posting lists locations into one super-set\n",
    "        super_posting_locs = defaultdict(list)\n",
    "        for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
    "            if not blob.name.endswith(\"pickle\"):\n",
    "                continue\n",
    "            with blob.open(\"rb\") as f:\n",
    "                posting_locs = pickle.load(f)\n",
    "                for k, v in posting_locs.items():\n",
    "                    super_posting_locs[k].extend(v)\n",
    "        # create inverted index\n",
    "        inverted_index_titles = inverted_index_gcp.InvertedIndex(doc_pairs_dict=doc_title_pairs_dict, bucket_name=bucket_name)        \n",
    "        inverted_index_titles.posting_locs = super_posting_locs\n",
    "        w2df = calculate_df(postings)\n",
    "        w2df_dict = w2df.collectAsMap()\n",
    "        inverted_index_titles.df = w2df_dict\n",
    "        for w, pl in inverted_index_titles.posting_lists_iter():\n",
    "            result = 0\n",
    "            for posting in pl:\n",
    "                if posting[0] not in inverted_index_titles.DL:\n",
    "                    inverted_index_titles.DL[posting[0]] = posting[1]\n",
    "                else:\n",
    "                    inverted_index_titles.DL[posting[0]] += posting[1]\n",
    "                result += posting[1]\n",
    "            inverted_index_titles.term_total[w] = result\n",
    "        # write to memory each inverted index\n",
    "        inverted_index_titles.write_index('.', \"title_index\")\n",
    "        index_src = \"title_index.pkl\"\n",
    "        index_dst = f'gs://{bucket_name}/title_index/{index_src}'\n",
    "        subprocess.run(['gsutil', 'cp', index_src, index_dst])\n",
    "\n",
    "    # body\n",
    "    if name == 'body':\n",
    "        # bucket for body\n",
    "        bucket_name = 'body_bucket_roy_dudi_stemmed'\n",
    "        # use title for names returns\n",
    "        # stemm\n",
    "        doc_pairs = doc_pairs.map(lambda x :(x[1],stemmer_tokens(x[0])))\n",
    "        doc_title_pairs_ = title_pairs.collect()\n",
    "        doc_title_pairs_dict = {element[1]: element[0] for element in doc_title_pairs_}\n",
    "        # get posting list\n",
    "        word_counts = doc_pairs.flatMap(lambda x: word_count(x[1], x[0]))\n",
    "        postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "        postings_filtered = postings.filter(lambda x: len(x[1])>50)\n",
    "        # partition posting lists and write out\n",
    "        _ = partition_postings_and_write(postings_filtered, bucket_name).collect()\n",
    "        # collect all posting lists locations into one super-set\n",
    "        super_posting_locs = defaultdict(list)\n",
    "        for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
    "            if not blob.name.endswith(\"pickle\"):\n",
    "                continue\n",
    "            with blob.open(\"rb\") as f:\n",
    "                posting_locs = pickle.load(f)\n",
    "                for k, v in posting_locs.items():\n",
    "                    super_posting_locs[k].extend(v)\n",
    "        # create inverted index\n",
    "        inverted_index_text = inverted_index_gcp.InvertedIndex(doc_pairs_dict=doc_title_pairs_dict, bucket_name=bucket_name)\n",
    "        w2df = calculate_df(postings_filtered)\n",
    "        w2df_dict = w2df.collectAsMap()\n",
    "        inverted_index_text.df = w2df_dict\n",
    "        inverted_index_text.posting_locs = super_posting_locs\n",
    "        for w, pl in inverted_index_text.posting_lists_iter():\n",
    "            result = 0\n",
    "            for posting in pl:\n",
    "                if posting[0] not in inverted_index_text.DL:\n",
    "                    inverted_index_text.DL[posting[0]] = posting[1]\n",
    "                else:\n",
    "                    inverted_index_text.DL[posting[0]] += posting[1]\n",
    "                result += posting[1]\n",
    "            inverted_index_text.term_total[w] = result\n",
    "        # write to memory each inverted index\n",
    "        inverted_index_text.write_index('.', \"body_index\")\n",
    "        index_src = \"body_index.pkl\"\n",
    "        index_dst = f'gs://{bucket_name}/body_index/{index_src}'\n",
    "        subprocess.run(['gsutil', 'cp', index_src, index_dst])\n",
    "\n",
    "\n",
    "    # anchor\n",
    "    if name == 'anchor':\n",
    "        # bucket for anchor\n",
    "        bucket_name = 'anchor_bucket_roy_dudi_stemmed'\n",
    "        # use title for names returs\n",
    "        # stemm\n",
    "        doc_pairs = doc_pairs.map(lambda x :(x[0],stemmer_tokens(x[1])))\n",
    "        doc_title_pairs_ = title_pairs.collect()\n",
    "        doc_title_pairs_dict = {element[1]: element[0] for element in doc_title_pairs_}\n",
    "        # get posting list\n",
    "        pages_links_flat = pages_links.flatMap(lambda x: x[1])\n",
    "        word_counts = pages_links_flat.flatMap(lambda x: word_count_bin(x[1], x[0]))\n",
    "        postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "        # partition posting lists and write out\n",
    "        _ = partition_postings_and_write(postings, bucket_name).collect()\n",
    "        # collect all posting lists locations into one super-set\n",
    "        super_posting_locs = defaultdict(list)\n",
    "        for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
    "            if not blob.name.endswith(\"pickle\"):\n",
    "                continue\n",
    "            with blob.open(\"rb\") as f:\n",
    "                posting_locs = pickle.load(f)\n",
    "                for k, v in posting_locs.items():\n",
    "                    super_posting_locs[k].extend(v)\n",
    "        # create inverted index\n",
    "        inverted_index_anchor = inverted_index_gcp.InvertedIndex(doc_pairs_dict=doc_title_pairs_dict, bucket_name=bucket_name)\n",
    "        inverted_index_anchor.posting_locs = super_posting_locs\n",
    "        w2df = calculate_df(postings)\n",
    "        w2df_dict = w2df.collectAsMap()\n",
    "        inverted_index_anchor.df = w2df_dict\n",
    "        for w, pl in inverted_index_anchor.posting_lists_iter():\n",
    "            result = 0\n",
    "            for posting in pl:\n",
    "                if posting[0] not in inverted_index_anchor.DL:\n",
    "                    inverted_index_anchor.DL[posting[0]] = posting[1]\n",
    "                else:\n",
    "                    inverted_index_anchor.DL[posting[0]] += posting[1]\n",
    "                result += posting[1]\n",
    "            inverted_index_anchor.term_total[w] = result\n",
    "        # write to memory each inverted index\n",
    "        inverted_index_anchor.write_index('.', \"anchor_index\")\n",
    "        index_src = \"anchor_index.pkl\"\n",
    "        index_dst = f'gs://{bucket_name}/anchor_index/{index_src}'\n",
    "        subprocess.run(['gsutil', 'cp', index_src, index_dst])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcWEM00dECw6"
   },
   "outputs": [],
   "source": [
    "write_data_stemmed(\"title\", doc_title_pairs, doc_title_pairs)\n",
    "write_data_stemmed(\"body\", doc_text_pairs, doc_title_pairs)\n",
    "write_data_stemmed(\"anchor\", pages_links, doc_title_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItAUntHMAPFT"
   },
   "outputs": [],
   "source": [
    "def write_data(name, doc_pairs, title_pairs):\n",
    "    # title\n",
    "    if name == 'title':\n",
    "        # bucket for title\n",
    "        bucket_name = 'title_bucket_roy_dudi'\n",
    "        # convert rdd to list\n",
    "        doc_title_pairs_ = doc_pairs.collect()\n",
    "        # convert list to dict{id:text}\n",
    "        doc_title_pairs_dict = {element[1]: element[0] for element in doc_title_pairs_}\n",
    "        # tokenize the dicts\n",
    "        # doc_data_titles = {k: tokenize_bin(v) for k, v in doc_title_pairs_dict.items() if len(tokenize_bin(v)) != 0}        \n",
    "        # get posting list\n",
    "        word_counts = doc_pairs.flatMap(lambda x: word_count_bin(x[0], x[1]))\n",
    "        postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "        # partition posting lists and write out\n",
    "        _ = partition_postings_and_write(postings, bucket_name).collect()\n",
    "        # collect all posting lists locations into one super-set\n",
    "        super_posting_locs = defaultdict(list)\n",
    "        for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
    "            if not blob.name.endswith(\"pickle\"):\n",
    "                continue\n",
    "            with blob.open(\"rb\") as f:\n",
    "                posting_locs = pickle.load(f)\n",
    "                for k, v in posting_locs.items():\n",
    "                    super_posting_locs[k].extend(v)\n",
    "        # create inverted index\n",
    "        inverted_index_titles = inverted_index_gcp.InvertedIndex(doc_pairs_dict=doc_title_pairs_dict, bucket_name=bucket_name)        \n",
    "        inverted_index_titles.posting_locs = super_posting_locs\n",
    "        w2df = calculate_df(postings)\n",
    "        w2df_dict = w2df.collectAsMap()\n",
    "        inverted_index_titles.df = w2df_dict\n",
    "        for w, pl in inverted_index_titles.posting_lists_iter():\n",
    "            result = 0\n",
    "            for posting in pl:\n",
    "                if posting[0] not in inverted_index_titles.DL:\n",
    "                    inverted_index_titles.DL[posting[0]] = posting[1]\n",
    "                else:\n",
    "                    inverted_index_titles.DL[posting[0]] += posting[1]\n",
    "                result += posting[1]\n",
    "            inverted_index_titles.term_total[w] = result\n",
    "        # write to memory each inverted index\n",
    "        inverted_index_titles.write_index('.', \"title_index\")\n",
    "        index_src = \"title_index.pkl\"\n",
    "        index_dst = f'gs://{bucket_name}/title_index/{index_src}'\n",
    "        subprocess.run(['gsutil', 'cp', index_src, index_dst])\n",
    "\n",
    "    # body\n",
    "    if name == 'body':\n",
    "        # bucket for body\n",
    "        bucket_name = 'body_bucket_roy_dudi'\n",
    "        # use title for names returns\n",
    "        doc_title_pairs_ = title_pairs.collect()\n",
    "        doc_title_pairs_dict = {element[1]: element[0] for element in doc_title_pairs_}\n",
    "        # get posting list\n",
    "        word_counts = doc_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
    "        postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "        postings_filtered = postings.filter(lambda x: len(x[1])>50)\n",
    "        # partition posting lists and write out\n",
    "        _ = partition_postings_and_write(postings_filtered, bucket_name).collect()\n",
    "        # collect all posting lists locations into one super-set\n",
    "        super_posting_locs = defaultdict(list)\n",
    "        for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
    "            if not blob.name.endswith(\"pickle\"):\n",
    "                continue\n",
    "            with blob.open(\"rb\") as f:\n",
    "                posting_locs = pickle.load(f)\n",
    "                for k, v in posting_locs.items():\n",
    "                    super_posting_locs[k].extend(v)\n",
    "        # create inverted index\n",
    "        inverted_index_text = inverted_index_gcp.InvertedIndex(doc_pairs_dict=doc_title_pairs_dict, bucket_name=bucket_name)\n",
    "        w2df = calculate_df(postings_filtered)\n",
    "        w2df_dict = w2df.collectAsMap()\n",
    "        inverted_index_text.df = w2df_dict\n",
    "        inverted_index_text.posting_locs = super_posting_locs\n",
    "        for w, pl in inverted_index_text.posting_lists_iter():\n",
    "            result = 0\n",
    "            for posting in pl:\n",
    "                if posting[0] not in inverted_index_text.DL:\n",
    "                    inverted_index_text.DL[posting[0]] = posting[1]\n",
    "                else:\n",
    "                    inverted_index_text.DL[posting[0]] += posting[1]\n",
    "                result += posting[1]\n",
    "            inverted_index_text.term_total[w] = result\n",
    "        # write to memory each inverted index\n",
    "        inverted_index_text.write_index('.', \"body_index\")\n",
    "        index_src = \"body_index.pkl\"\n",
    "        index_dst = f'gs://{bucket_name}/body_index/{index_src}'\n",
    "        subprocess.run(['gsutil', 'cp', index_src, index_dst])\n",
    "\n",
    "\n",
    "\n",
    "    # anchor\n",
    "    if name == 'anchor':\n",
    "        # bucket for anchor\n",
    "        bucket_name = 'anchor_bucket_roy_dudi'\n",
    "        # use title for names returs\n",
    "        doc_title_pairs_ = title_pairs.collect()\n",
    "        doc_title_pairs_dict = {element[1]: element[0] for element in doc_title_pairs_}\n",
    "        # get posting list\n",
    "        pages_links_flat = pages_links.flatMap(lambda x: x[1])\n",
    "        word_counts = pages_links_flat.flatMap(lambda x: word_count_bin(x[1], x[0]))\n",
    "        postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
    "        # partition posting lists and write out\n",
    "        _ = partition_postings_and_write(postings, bucket_name).collect()\n",
    "        # collect all posting lists locations into one super-set\n",
    "        super_posting_locs = defaultdict(list)\n",
    "        for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
    "            if not blob.name.endswith(\"pickle\"):\n",
    "                continue\n",
    "            with blob.open(\"rb\") as f:\n",
    "                posting_locs = pickle.load(f)\n",
    "                for k, v in posting_locs.items():\n",
    "                    super_posting_locs[k].extend(v)\n",
    "        # create inverted index\n",
    "        inverted_index_anchor = inverted_index_gcp.InvertedIndex(doc_pairs_dict=doc_title_pairs_dict, bucket_name=bucket_name)\n",
    "        inverted_index_anchor.posting_locs = super_posting_locs\n",
    "        w2df = calculate_df(postings)\n",
    "        w2df_dict = w2df.collectAsMap()\n",
    "        inverted_index_anchor.df = w2df_dict\n",
    "        for w, pl in inverted_index_anchor.posting_lists_iter():\n",
    "            result = 0\n",
    "            for posting in pl:\n",
    "                if posting[0] not in inverted_index_anchor.DL:\n",
    "                    inverted_index_anchor.DL[posting[0]] = posting[1]\n",
    "                else:\n",
    "                    inverted_index_anchor.DL[posting[0]] += posting[1]\n",
    "                result += posting[1]\n",
    "            inverted_index_anchor.term_total[w] = result\n",
    "        # write to memory each inverted index\n",
    "        inverted_index_anchor.write_index('.', \"anchor_index\")\n",
    "        index_src = \"anchor_index.pkl\"\n",
    "        index_dst = f'gs://{bucket_name}/anchor_index/{index_src}'\n",
    "        subprocess.run(['gsutil', 'cp', index_src, index_dst])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
