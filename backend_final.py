# -*- coding: utf-8 -*-
"""backend_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PStJmnKrPjGkjX3qVdM2e2DeCA-_fcmb

# imports
"""

# Install a particular version of `google-cloud-storage` because (oddly enough) 
# the  version on Colab and GCP is old. A dependency error below is okay.
import subprocess

# if the following command generates an error, you probably didn't enable
# the cluster security option "Allow API access to all Google Cloud services"
# under Manage Security â†’ Project Access when setting up the cluster


# imports
import sys
from collections import Counter, OrderedDict
import itertools
from itertools import islice, count, groupby
import pandas as pd
import os
import re
from operator import itemgetter
import nltk
from nltk.stem.porter import *
from nltk.corpus import stopwords
from time import time
from timeit import timeit
from pathlib import Path
import pickle
import pandas as pd
import numpy as np
from google.cloud import storage
import pyspark
from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark import SparkContext, SparkConf
from pyspark.sql import SQLContext
from pyspark.ml.feature import Tokenizer, RegexTokenizer
from graphframes import *
import signal
import gcsfs
import requests
import bs4
import hashlib
import inverted_index_gcp
from sklearn.metrics.pairwise import cosine_similarity
from scipy import sparse
import numpy as np
from numpy.linalg import norm

nltk.download('stopwords')

client = storage.Client()

"""# All functions"""

def _hash(s):
    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()


NUM_BUCKETS = 124
def token2bucket_id(token):
    return int(_hash(token),16) % NUM_BUCKETS


def partition_postings_and_write(postings,bucket_name):
    # YOUR CODE HERE
    def to_list(a):
        return [a]

    def append(a, b):
        a.append(b)
        return a

    def extend(a, b):
        a.extend(b)
        return a
    bucket_id_rdd = postings.map(lambda x: (token2bucket_id(x[0]), (x[0], x[1])))
    write_list_rdd = bucket_id_rdd.combineByKey(to_list, append, extend).map(
        lambda x: inverted_index_gcp.InvertedIndex.write_a_posting_list(x, bucket_name))
    return write_list_rdd


from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict, Counter
import re
import nltk
import pickle
import numpy as np

nltk.download('stopwords')

from nltk.corpus import stopwords
from tqdm import tqdm
import operator
from itertools import islice, count
from contextlib import closing

import json
from io import StringIO
from pathlib import Path
from operator import itemgetter
import pickle
import matplotlib.pyplot as plt
import builtins


# TUPLE_SIZE = 6  # We're going to pack the doc_id and tf values in this
# # many bytes.
# TF_MASK = 2 ** 16 - 1  # Masking the 16 low bits of an integer

# DL = {}  # We're going to update and calculate this after each document. This will be usefull for the calculation of AVGDL (utilized in BM25)


from logging import exception

RE_WORD = re.compile(r"""[\#\@\w](['\-]?\w){2,24}""", re.UNICODE)
stopwords_frozen = frozenset(stopwords.words('english'))

corpus_stopwords = ["category", "references", "also", "external", "links", 
                    "may", "first", "see", "history", "people", "one", "two", 
                    "part", "thumb", "including", "second", "following", 
                    "many", "however", "would", "became"]

all_stopwords = stopwords_frozen.union(corpus_stopwords)


def tokenize(text):
    """
    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.

    Parameters:
    -----------
    text: string , represting the text to tokenize.

    Returns:
    -----------
    list of tokens (e.g., list of tokens).
    """
    RE_WORD = re.compile(r"""[\#\@\w](['\-]?\w){2,24}""", re.UNICODE)
    list_of_tokens = [token.group() for token in RE_WORD.finditer(text.lower()) if
                      token.group() not in all_stopwords]
    if len(list_of_tokens) == 0:
        RE_WORD = re.compile(r"""[\#\@\w](['\-]?\w){0,24}""", re.UNICODE)
        list_of_tokens = [token.group() for token in RE_WORD.finditer(text.lower())]
    return list_of_tokens


def tokenize_word2vec(text):
    RE_WORD = re.compile(r"""[\#\@\w](['\-]?\w){0,24}""", re.UNICODE)
    if re.search(r'\d', text):
        return []
    list_of_tokens = [token.group() for token in RE_WORD.finditer(text.lower())]
    return list_of_tokens
    return list_of_tokens


def generate_query_tfidf_vector(query_to_search, index):
    """
    Generate a vector representing the query. Each entry within this vector represents a tfidf score.
    The terms representing the query will be the unique terms in the index.

    We will use tfidf on the query as well.
    For calculation of IDF, use log with base 10.
    tf will be normalized based on the length of the query.

    Parameters:
    -----------
    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.').
                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']

    index:           inverted index loaded from the corresponding files.

    Returns:
    -----------
    vectorized query with tfidf scores
    """

    epsilon = .0000001
    total_vocab_size = len(index.term_total)
    Q = np.zeros((total_vocab_size))
    term_vector = list(index.term_total.keys())
    counter = Counter(query_to_search)
    for token in np.unique(query_to_search):
        if token in index.term_total.keys():  # avoid terms that do not appear in the index.
            tf = counter[token] / len(query_to_search)  # term frequency divded by the length of the query
            df = index.df[token]
            idf = math.log((len(index.DL)) / (df + epsilon), 10)  # smoothing

            try:
                ind = term_vector.index(token)
                Q[ind] = tf * idf
            except:
                pass
    return Q


def get_posting_iter(index):
    """
    This function returning the iterator working with posting list.

    Parameters:
    ----------
    index: inverted index
    """
    words, pls = zip(*index.posting_lists_iter())
    return words, pls


def get_idate_documents_and_scores(query_to_search, index, words, bucket_name):
    """
    Generate a dictionary representing a pool of candidate documents for a given query. This function will go through every token in query_to_search
    and fetch the corresponding information (e.g., term frequency, document frequency, etc.') needed to calculate TF-IDF from the posting list.
    Then it will populate the dictionary 'candidates.'
    For calculation of IDF, use log with base 10.
    tf will be normalized based on the length of the document.

    Parameters:
    -----------
    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.').
                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']

    index:           inverted index loaded from the corresponding files.

    words,pls: iterator for working with posting.

    Returns:
    -----------
    dictionary of candidates. In the following format:
                                                               key: pair (doc_id,term)
                                                               value: tfidf score.
    """
    candidates = {}
    real_candidates = {}
    for term in np.unique(query_to_search):
        if term in words:
            list_of_doc = index.read_posting_list(index, term, bucket_name)
            for doc_id, freq in list_of_doc:
                if doc_id not in candidates:
                    candidates[doc_id] = int(freq)
                else:
                    candidates[doc_id] += int(freq)

    return np.unique([doc_id for doc_id, freq in candidates.items() if freq > (50 * len(query_to_search))])

            # normlized_tfidf = [(doc_id, (freq / index.DL[doc_id]) * math.log(len(index.DL) / index.df[term], 10)) for doc_id, freq
            #                    in list_of_doc]
            #
            # for doc_id, tfidf in normlized_tfidf:
            #     candidates[(doc_id, term)] = candidates.get((doc_id, term), 0) + tfidf
    # return candidates


def generate_document_tfidf_matrix(query_to_search, index, words, pls):
    """
    Generate a DataFrame `D` of tfidf scores for a given query.
    Rows will be the documents candidates for a given query
    Columns will be the unique terms in the index.
    The value for a given document and term will be its tfidf score.

    Parameters:
    -----------
    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.').
                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']

    index:           inverted index loaded from the corresponding files.


    words,pls: iterator for working with posting.

    Returns:
    -----------
    DataFrame of tfidf scores.
    """

    total_vocab_size = len(index.term_total)
    candidates_scores = get_candidate_documents_and_scores(query_to_search, index, words,
                                                           pls)  # We do not need to utilize all document. Only the docuemnts which have corrspoinding terms with the query.
    unique_candidates = np.unique([doc_id for doc_id, freq in candidates_scores.keys()])
    D = np.zeros((len(unique_candidates), total_vocab_size))
    D = pd.DataFrame(D)

    D.index = unique_candidates
    D.columns = index.term_total.keys()

    for key in candidates_scores:
        tfidf = candidates_scores[key]
        doc_id, term = key
        D.loc[doc_id][term] = tfidf

    return D


import numpy as np


# Put your `generate_graph` function here
def generate_graph(pages):
  # YOUR CODE HERE
  edges = pages.flatMapValues(lambda x: x).map(lambda x: (x[0],x[1][0])).distinct()
  vertices1 = edges.values()
  vertices2 = edges.keys()
  vertices = vertices1.union(vertices2).distinct().map(lambda x: (x,x))
  return edges, vertices


def get_top_n(sim_dict, N=100):
    """
    Sort and return the highest N documents according to the cosine similarity score.
    Generate a dictionary of cosine similarity scores

    Parameters:
    -----------
    sim_dict: a dictionary of similarity score as follows:
                                                                key: document id (e.g., doc_id)
                                                                value: similarity score. We keep up to 5 digits after the decimal point. (e.g., round(score,5))

    N: Integer (how many documents to retrieve). By default N = 3

    Returns:
    -----------
    a ranked list of pairs (doc_id, score) in the length of N.
    """

    return builtins.sorted([(doc_id, score) for doc_id, score in sim_dict.items()],
                           key=lambda x: x[1], reverse=True)[:N]


def get_topN_score_for_queries(queries_to_search, index, N=3):
    """
    Generate a dictionary that gathers for every query its topN score.

    Parameters:
    -----------
    queries_to_search: a dictionary of queries as follows:
                                                        key: query_id
                                                        value: list of tokens.
    index:           inverted index loaded from the corresponding files.
    N: Integer. How many documents to retrieve. This argument is passed to the topN function. By default N = 3, for the topN function.

    Returns:
    -----------
    return: a dictionary of queries and topN pairs as follows:
                                                        key: query_id
                                                        value: list of pairs in the following format:(doc_id, score).
    """
    # YOUR CODE HERE
    dict1 = {}
    d = get_posting_iter(index)
    i = 0
    for key, query in queries_to_search.items():
        i += 1
        Q = generate_query_tfidf_vector(query, index)
        D = generate_document_tfidf_matrix(query, index, d[0], d[1])
        x = fast_cosine_similarity(D, Q)
        z = get_top_n(x, N)
        dict1[key] = z

    return dict1


import math
from itertools import chain
import time


# When preprocessing the data have a dictionary of document length for each document saved in a variable called `DL`.
# class BM25_from_index:
#     """
#     Best Match 25.
#     ----------
#     k1 : float, default 1.5
#
#     b : float, default 0.75
#
#     index: inverted index
#     """
#
#     def __init__(self, index, bucket_name, k1=1.5, b=0.75):
#         self.b = b
#         self.k1 = k1
#         self.index = index
#         self.N = len(index.DL)
#         self.AVGDL = builtins.sum(index.DL.values()) / self.N
#         # self.words, self.pls = zip(*self.index.posting_lists_iter())
#         self.words = index.posting_locs
#         self.bucket_name = bucket_name
#
#
#     def calc_idf(self, list_of_tokens):
#         """
#         This function calculate the idf values according to the BM25 idf formula for each term in the query.
#
#         Parameters:
#         -----------
#         query: list of token representing the query. For example: ['look', 'blue', 'sky']
#
#         Returns:
#         -----------
#         idf: dictionary of idf scores. As follows:
#                                                     key: term
#                                                     value: bm25 idf score
#         """
#         idf = {}
#         for term in list_of_tokens:
#             if term in self.words:
#                 n_ti = self.index.df[term]
#                 idf[term] = math.log(1 + (self.N - n_ti + 0.5) / (n_ti + 0.5))
#             else:
#                 pass
#         return idf
#
#     def search(self, queries, N=100):
#         """
#         This function calculate the bm25 score for given query and document.
#         We need to check only documents which are 'candidates' for a given query.
#         This function return a dictionary of scores as the following:
#                                                                     key: query_id
#                                                                     value: a ranked list of pairs (doc_id, score) in the length of N.
#
#         Parameters:
#         -----------
#         query: list of token representing the query. For example: ['look', 'blue', 'sky']
#         doc_id: integer, document id.
#
#         Returns:
#         -----------
#         score: float, bm25 score.
#         """
#         # YOUR CODE HERE
#         res = {}
#         for query_id in queries:
#             self.idf = self.calc_idf(queries[query_id])
#             c = get_candidate_documents_and_scores(queries[query_id], self.index, self.words, self.bucket_name)
#             sim = {doc: self._score(queries[query_id], doc) for doc in c}
#
#             # cd = np.unique([doc[0] for doc in c.keys()])
#
#             # candidates = get_candidate_documents(query_to_search, self.index, self.f_name)
#
#             res[query_id] = get_top_n(sim, N)
#         return res
#
#
#     def _score(self, query, doc_id):
#         """
#         This function calculate the bm25 score for given query and document.
#
#         Parameters:
#         -----------
#         query: list of token representing the query. For example: ['look', 'blue', 'sky']
#         doc_id: integer, document id.
#
#         Returns:
#         -----------
#         score: float, bm25 score.
#         """
#         score = 0.0
#         doc_len = self.index.DL[doc_id]
#         for term in query:
#             if term in self.words:
#                 term_frequencies = dict(self.index.read_posting_list(self.index, term, self.bucket_name))
#                 if doc_id in term_frequencies.keys():
#                     freq = term_frequencies[doc_id]
#                     numerator = self.idf[term] * freq * (self.k1 + 1)
#                     denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.AVGDL)
#                     if doc_id in page_rank_dict:
#                         score += (numerator / denominator) + math.log(page_rank_dict[doc_id])
#                     else:
#                         score += (numerator / denominator)
#         return score


def calculate_df(postings):
  # YOUR CODE HERE
  result = postings.map(lambda x: (x[0],len(x[1])))
  return result


def get_id_name_list_dict(score_dict, doc_title_pairs_dict):
    res = []
    for doc_id in score_dict:
        if doc_id in doc_title_pairs_dict:
            res.append((int(doc_id), doc_title_pairs_dict[doc_id]))
        else:
            url = "https://en.wikipedia.org/?curid="
            url += str(doc_id)
            response = requests.get(url=url, )
            soup = bs4.BeautifulSoup(response.content, 'html.parser')
            title = soup.find(id="firstHeading")
            res.append((int(doc_id), str(title.text)))
    return res

def get_id_name_list(half_and_half, doc_title_pairs_dict):
    res = []
    for tup in half_and_half:
        if tup[0] in doc_title_pairs_dict:
            res.append((int(tup[0]), doc_title_pairs_dict[tup[0]]))
        else:
            url = "https://en.wikipedia.org/?curid="
            url += str(tup[0])
            response = requests.get(url=url, )
            soup = bs4.BeautifulSoup(response.content, 'html.parser')
            title = soup.find(id="firstHeading")
            res.append((int(tup[0]), str(title.text)))
    return res


def tokenize_bin(text):
    """
    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.

    Parameters:
    -----------
    text: string , represting the text to tokenize.

    Returns:
    -----------
    list of tokens (e.g., list of tokens).
    """
    RE_WORD = re.compile(r"""[\#\@\w](['\-]?\w){2,24}""", re.UNICODE)
    list_of_tokens = set([token.group() for token in RE_WORD.finditer(text.lower()) if
                      token.group() not in all_stopwords])
    if len(list_of_tokens) == 0:
        RE_WORD = re.compile(r"""[\#\@\w](['\-]?\w){0,24}""", re.UNICODE)
        list_of_tokens = set([token.group() for token in RE_WORD.finditer(text.lower())])
    return list(list_of_tokens)


def reduce_word_counts(unsorted_pl):
    lst = sorted(unsorted_pl, key=lambda x:x[0])
    return lst


def word_count(text, id):
  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]
  # YOUR CODE HERE
  tokens_without_stopword = [word for word in tokens if word not in all_stopwords]
  result_dict = {}
  if len(tokens_without_stopword) == 0:
    tokens_without_stopword = tokens
  for word in tokens_without_stopword:
    if word not in result_dict:
      result_dict[word] = [id,1]
    else:
      result_dict[word][1] += 1
  map_reduce_res = [(key,tuple(result_dict[key])) for key in result_dict]
  return map_reduce_res


def word_count_bin(text, id):
    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]
    lst_words = []
    cnt = Counter(i for i in tokens if i not in all_stopwords)
    if len(cnt) == 0:
      cnt = Counter(i for i in tokens)
    for i in cnt.keys():
        lst_words.append((i,(id,1)))
    return lst_words


# Getting tokens from the text while removing punctuations.
def stemmer_tokens(tokens):
  # YOUR CODE HERE
  stemmer = PorterStemmer()
  tokens = tokens.split(" ")
  tokens_removed = [word for word in tokens if word not in all_stopwords]
  if len(tokens_removed) == 0:
    tokens_removed = tokens
  tokens_stemmed = [stemmer.stem(word) for word in tokens_removed]
  tokens_stemmed = ' '.join(tokens_stemmed)
  return tokens_stemmed


def fast_super_duper_cosine(query, index, bucket_name):
  candidates = {}
  tfidf_dict = {}
  tf_query = Counter(query)
  query_tfidf_list = []
  words = index.posting_locs

  for term in np.unique(query):
    query_tfidf = (tf_query[term] / len(query)) * (np.log10(len(index.DL)) / index.df[term])
    query_tfidf_list.append(query_tfidf)
    if term in words:
      list_of_doc = index.read_posting_list(index, term, bucket_name)
      for doc_id, freq in list_of_doc:
        DOC_tfidf = (freq / index.DL[doc_id]) * np.log10(len(index.DL) / index.df[term])
        if doc_id not in candidates:
          tfidf_dict[doc_id] = DOC_tfidf * query_tfidf
        else:
          tfidf_dict[doc_id] += DOC_tfidf * query_tfidf
        if doc_id not in candidates:
          candidates[doc_id] = freq
        else:
          candidates[doc_id] += freq
  # create norma for query
  norma_query = np.sum(np.square(np.array(query_tfidf_list)))

  for doc_id, freq in candidates.items():
    if freq > (50 * len(query)):
      norma = np.square(np.square(doc_tf_idf[doc_id])) * np.square(norma_query)
      candidates[doc_id] = tfidf_dict[doc_id] / norma
  return {k: v for k, v in sorted(candidates.items(), key=lambda item: item[1], reverse=True)[:100]}


def fast_super_duper_cosine_main(query, index, bucket_name):
  candidates = {}
  tfidf_dict = {}
  tf_query = Counter(query)
  query_tfidf_list = []
  words = index.posting_locs

  for term in np.unique(query):
    query_tfidf = (tf_query[term] / len(query)) * (np.log10(len(index.DL)) / index.df[term])
    query_tfidf_list.append(query_tfidf)
    if term in words:
      list_of_doc = index.read_posting_list(index, term, bucket_name)
      for doc_id, freq in list_of_doc:
        DOC_tfidf = (freq / index.DL[doc_id]) * np.log10(len(index.DL) / index.df[term])
        if doc_id not in candidates:
          tfidf_dict[doc_id] = DOC_tfidf * query_tfidf
        else:
          tfidf_dict[doc_id] += DOC_tfidf * query_tfidf
        if doc_id not in candidates:
          candidates[doc_id] = freq
        else:
          candidates[doc_id] += freq
  # create norma for query
  norma_query = np.sum(np.square(np.array(query_tfidf_list)))

  for doc_id, freq in candidates.items():
    if freq > (50 * len(query)):
      norma = np.square(np.square(doc_tf_idf[doc_id])) * np.square(norma_query)
      candidates[doc_id] = tfidf_dict[doc_id] / norma
  return {k: v for k, v in sorted(candidates.items(), key=lambda item: item[1], reverse=True)[:10]}


def merge_results(title_scores, body_scores, anchor_scores, title_weight=0.5, text_weight=0.25, anchor_weight=0.25, N=100):
    """
    This function merge and sort documents retrieved by its weighte score (e.g., title and body).

    Parameters:
    -----------
    title_scores: a dictionary build upon the title index of queries and tuples representing scores as follows:
                                                                            key: query_id
                                                                            value: list of pairs in the following format:(doc_id,score)

    body_scores: a dictionary build upon the body/text index of queries and tuples representing scores as follows:
                                                                            key: query_id
                                                                            value: list of pairs in the following format:(doc_id,score)
    title_weight: float, for weigted average utilizing title and body scores
    text_weight: float, for weigted average utilizing title and body scores
    N: Integer. How many document to retrieve. This argument is passed to topN function. By default N = 3, for the topN function.

    Returns:
    -----------
    dictionary of querires and topN pairs as follows:
                                                        key: query_id
                                                        value: list of pairs in the following format:(doc_id,score).
    """

    #  {doc id : score}
    # YOUR CODE HERE
    def get_top_n_new(sim_dict, N):
        return {k: v for k, v in sorted([(doc_id, score) for doc_id, score in sim_dict.items()], key=lambda x: x[1], reverse=True)[:N]}

    #   d={}
    res = {}
    for doc_id, value in title_scores.items():
        res[doc_id] = [title_weight * value]
    for doc_id, value in body_scores.items():
        if doc_id in res:
            res[doc_id] += [text_weight * value]
        else:
            res[doc_id] = [text_weight * value]
    for doc_id, value in anchor_scores.items():
        if doc_id in res:
            res[doc_id] += [anchor_weight * value]
        else:
            res[doc_id] = [anchor_weight * value]
    x = get_top_n_new(res, 100)
    return x


def search_binary(query,index, bucket_name):
  scores = {}
  words = index.posting_locs
  for term in query:
    if term in words:
        list_of_doc = index.read_posting_list(index, term, bucket_name)
        for doc_id, freq in list_of_doc:
          if doc_id not in scores:
            scores[doc_id] = 1
          else:
            scores[doc_id] += 1
  return sorted(scores.items(), key=lambda x: x[1], reverse=True)

# def search_binary(query,index, bucket_name):
#   scores = {}
#   words = index.posting_locs
#   query=set(query)
#   len_query=len(query)
#   res={}
#   for term in query:
#     if term in words:
#       #(dic id, freq)
#         list_of_doc = index.read_posting_list(index, term, bucket_name)
#         #scores[doc_id]=(query.intersection(list_of_doc)) /(query.union(list_of_doc))
#         # (doc_id,freq)
#         for doc_id, freq in list_of_doc:
#           if doc_id not in scores:
#             scores[doc_id] = 1
#           else:
#             scores[doc_id] += 1
#   for doc_id, value in scores.items():
#     res[doc_id] = value/(index.DL[doc_id]+len_query)
#   return sorted(res.items(), key=lambda x: x[1], reverse=True)


"""# Read index"""

def read_data(name):
    # title
    if name == 'title':
        # bucket for title
        bucket_name = 'title_bucket_roy_dudi'
        index_src = "title_index.pkl"
        index_dst = f'gs://{bucket_name}/title_index/{index_src}'
        # read
        # Set up the connection to the Google Cloud Storage bucket
        fs = gcsfs.GCSFileSystem()
        with fs.open(index_dst, 'rb') as f:
            # Load the pickle file
            idx_title = pickle.load(f)
        # with open('title_index.pkl', 'rb') as f:
        #     # Load the pickle file
        #     idx_title = pickle.load(f)
        # read posting lists from disk
        # title_words, title_pls = zip(*idx_title.posting_lists_iter())
        return (idx_title, idx_title.titles)

    # body
    if name == 'body':
        # bucket for body
        bucket_name = 'body_bucket_roy_dudi'
        index_src = "body_index.pkl"
        index_dst = f'gs://{bucket_name}/body_index/{index_src}'

        # read
        # Set up the connection to the Google Cloud Storage bucket
        fs = gcsfs.GCSFileSystem()
        with fs.open(index_dst, 'rb') as f:
            # Load the pickle file
            idx_body = pickle.load(f)
        # with open('body_index.pkl', 'rb') as f:
        #     # Load the pickle file
        #     idx_body = pickle.load(f)
        # read posting lists from disk
        # body_words, body_pls = zip(*idx_body.posting_lists_iter())
        return idx_body

    # anchor
    if name == 'anchor':
        # bucket for anchor
        bucket_name = 'anchor_bucket_roy_dudi'
        index_src = "anchor_index.pkl"
        index_dst = f'gs://{bucket_name}/anchor_index/{index_src}'
        # read
        # Set up the connection to the Google Cloud Storage bucket
        fs = gcsfs.GCSFileSystem()
        with fs.open(index_dst, 'rb') as f:
            # Load the pickle file
            idx_anchor = pickle.load(f)
        # with open('anchor_index.pkl', 'rb') as f:
            # # Load the pickle file
            # idx_anchor = pickle.load(f)
        # read posting lists from disk
        # anchor_words, anchor_pls = zip(*idx_anchor.posting_lists_iter())
        return idx_anchor

print("start to read index")
idx_title, doc_title_pairs_dict = read_data("title")
print("finished title")
idx_body = read_data("body")
print("finished body")
idx_anchor = read_data("anchor")
print("finished anchor")
print("finished to read index")

"""# Read stemmed index"""

def read_stemmed_data(name):
    # title
    if name == 'title':
        # bucket for title
        bucket_name = 'title_bucket_roy_dudi_stemmed'
        index_src = "title_index.pkl"
        index_dst = f'gs://{bucket_name}/title_index/{index_src}'
        # read
        # Set up the connection to the Google Cloud Storage bucket
        fs = gcsfs.GCSFileSystem()
        with fs.open(index_dst, 'rb') as f:
            # Load the pickle file
            idx_title = pickle.load(f)
        # read posting lists from disk
        title_words, title_pls = zip(*idx_title.posting_lists_iter())
        return (idx_title, title_words, title_pls, idx_title.titles)

    # body
    if name == 'body':
        # bucket for body
        bucket_name = 'body_bucket_roy_dudi_stemmed'
        index_src = "body_index.pkl"
        index_dst = f'gs://{bucket_name}/body_index/{index_src}'
        # read
        # Set up the connection to the Google Cloud Storage bucket
        fs = gcsfs.GCSFileSystem()
        with fs.open(index_dst, 'rb') as f:
            # Load the pickle file
            idx_body = pickle.load(f)
        # read posting lists from disk
        body_words, body_pls = zip(*idx_body.posting_lists_iter())
        return (idx_body, body_words, body_pls, idx_body.titles)

    # anchor
    if name == 'anchor':
        # bucket for anchor
        bucket_name = 'anchor_bucket_roy_dudi_stemmed'
        index_src = "anchor_index.pkl"
        index_dst = f'gs://{bucket_name}/anchor_index/{index_src}'
        # read
        # Set up the connection to the Google Cloud Storage bucket
        fs = gcsfs.GCSFileSystem()
        with fs.open(index_dst, 'rb') as f:
            # Load the pickle file
            idx_anchor = pickle.load(f)
        # read posting lists from disk
        anchor_words, anchor_pls = zip(*idx_anchor.posting_lists_iter())  
        return (idx_anchor, anchor_words, anchor_pls, idx_anchor.titles)

# idx_title_stemmed,title_words_stemmed,title_pls_stemmed,doc_title_pairs_dict = read_stemmed_data("title")
# idx_body_stemmed,body_words_stemmed,body_pls_stemmed,doc_title_pairs_dict = read_stemmed_data("body")
# idx_anchor_stemmed,anchor_words_stemmed,anchor_pls_stemmed,doc_title_pairs_dict = read_stemmed_data("anchor")

"""# read page rank"""

def read_page_rank():
  bucket_name = "help_files"
  page_rank_src = "page_rank.pkl"
  page_rank_dst = f'gs://{bucket_name}/{page_rank_src}'
  fs = gcsfs.GCSFileSystem()
  with fs.open(page_rank_dst, 'rb') as f:
      # Load the pickle file
      page_rank_dict = pickle.load(f)
  return dict(page_rank_dict)

print("start to read page rank")
page_rank_dict = read_page_rank()
print("finished to read page rank")

"""# read page views"""

def read_page_views():
  bucket_name = 'help_files'
  index_src = "page_views.pkl"
  index_dst = f'gs://{bucket_name}/{index_src}'
  fs = gcsfs.GCSFileSystem()
  with fs.open(index_dst, 'rb') as f:
      # Load the pickle file
      page_views_dict = pickle.load(f)
  return dict(page_views_dict)

print("start to read page views")
page_views_dict = read_page_views()
print("finished to read page views")

"""# read W2VEC"""

from gensim.models import word2vec
import logging
def read_w2vec():
  #Taken from: http://textminingonline.com/getting-started-with-word2vec-and-glove-in-python
  # text8 from: https://github.com/andersjo/word2vec_sampler
  bucket_name = 'help_files'
  model_dst = f'gs://{bucket_name}/model.pkl'
  fs = gcsfs.GCSFileSystem()
  with fs.open(model_dst, 'rb') as f:
      # Load the pickle file
      model = pickle.load(f)
  # download model
  client = storage.Client()
  bucket = client.get_bucket(bucket_name)
  blob = bucket.get_blob("text.model.pkl")
  blob.download_to_filename('./text.model.pkl')
  wiki_word2vec = model.wv.load_word2vec_format('text.model.pkl', binary=True)
  return wiki_word2vec

print("start to read w2vec")
wiki_word2vec = read_w2vec()
print("finished to read w2vec")

def read_norma():
  bucket_name = 'help_files'
  index_src = "doc_tf_idf.pkl"
  index_dst = f'gs://{bucket_name}/{index_src}'
  fs = gcsfs.GCSFileSystem()
  with fs.open(index_dst, 'rb') as f:
    # Load the pickle file
    doc_tf_idf = pickle.load(f)
  return dict(doc_tf_idf)

print("start to read norma")
doc_tf_idf = read_norma()
print("finished to read norma")
